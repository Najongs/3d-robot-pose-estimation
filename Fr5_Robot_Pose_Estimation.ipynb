{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8437d0",
   "metadata": {},
   "source": [
    "### FR5 Î°úÎ¥á Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ÑÏ≤òÎ¶¨ Í≥ºÏ†ï\n",
    "1. Ïπ¥Î©îÎùºÏôÄ Joint angleÏùò Time stamps ÎßûÏ∂îÍ∏∞\n",
    "2. ROIÏóê ÎßûÍ≤å Ï†ÅÏ†àÌïú label ÏÉùÏÑ± \n",
    "\n",
    "### FR5 Î°úÎ¥á ÏûêÏÑ∏ Ï∂îÏ†ï Í≥ºÏ†ï\n",
    "1. Ï†ÑÏ≤¥ Ïù¥ÎØ∏ÏßÄÏóêÏÑú ROI ÌÉêÏÉâ - SegFormer\n",
    "2. ROI Ïù¥ÎØ∏ÏßÄÏóêÏÑú Í∞Å Í¥ÄÏ†à Ï¢åÌëú Ï∂îÏ†ï 3Ï∞®Ïõê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659a55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Ï¥ù 7Í∞ú Îç∞Ïù¥ÌÑ∞ÏÖã ÏÑ∏Ìä∏ ÌÉêÏÉâ: ['/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_1st_250526', '/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_2nd_250526', '/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_3rd_250526', '/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_4th_250526', '/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_5th_250526', '/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_6th_250526', '/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_7th_250526']\n",
      "[Fr5_intertek_1st_250526] ÏôÑÎ£å: matched=1296 / images=1296 (too_far=0, no_joint=0) -> /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_1st_250526/matched_index.csv, /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_1st_250526/matched_index.jsonl\n",
      "[Fr5_intertek_2nd_250526] ÏôÑÎ£å: matched=1310 / images=1310 (too_far=0, no_joint=0) -> /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_2nd_250526/matched_index.csv, /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_2nd_250526/matched_index.jsonl\n",
      "[Fr5_intertek_3rd_250526] ÏôÑÎ£å: matched=1308 / images=1308 (too_far=0, no_joint=0) -> /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_3rd_250526/matched_index.csv, /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_3rd_250526/matched_index.jsonl\n",
      "[Fr5_intertek_4th_250526] ÏôÑÎ£å: matched=1310 / images=1310 (too_far=0, no_joint=0) -> /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_4th_250526/matched_index.csv, /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_4th_250526/matched_index.jsonl\n",
      "[Fr5_intertek_5th_250526] ÏôÑÎ£å: matched=1314 / images=1314 (too_far=0, no_joint=0) -> /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_5th_250526/matched_index.csv, /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_5th_250526/matched_index.jsonl\n",
      "[Fr5_intertek_6th_250526] ÏôÑÎ£å: matched=1296 / images=1296 (too_far=0, no_joint=0) -> /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_6th_250526/matched_index.csv, /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_6th_250526/matched_index.jsonl\n",
      "[Fr5_intertek_7th_250526] ÏôÑÎ£å: matched=1308 / images=1308 (too_far=0, no_joint=0) -> /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_7th_250526/matched_index.csv, /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_7th_250526/matched_index.jsonl\n",
      "üèóÔ∏è SegFormer Î™®Îç∏ Î°úÎî©: nvidia/mit-b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b2 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_540144/3405246827.py:348: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(segformer_model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Ï¥ù ÌååÎùºÎØ∏ÌÑ∞: 27,348,162\n",
      "üìä ÌõàÎ†® Í∞ÄÎä• ÌååÎùºÎØ∏ÌÑ∞: 27,348,162\n",
      "üèóÔ∏è SegFormer Î™®Îç∏ Î°úÎî©: nvidia/mit-b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b2 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_540144/3405246827.py:348: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(segformer_model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Ï¥ù ÌååÎùºÎØ∏ÌÑ∞: 27,348,162\n",
      "üìä ÌõàÎ†® Í∞ÄÎä• ÌååÎùºÎØ∏ÌÑ∞: 27,348,162\n",
      "Loaded pretrained ViT model: vit_base_patch16_224\n",
      "Starting training for 50 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]:   0%|          | 0/1959 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/najo/.conda/envs/dip/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/najo/.conda/envs/dip/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'RobotArmSegFKDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from bisect import bisect_left\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig\n",
    "import timm\n",
    "\n",
    "# ================= MKL Ï∂©Îèå Ìï¥Í≤∞ Î∞è Ïû•Ïπò ÏÑ§Ï†ï =================\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================= ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï =================\n",
    "ROOT = \"/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset\"\n",
    "MAX_TIME_DIFF = 0.025  # seconds\n",
    "# ==============================================\n",
    "\n",
    "# =========================================================\n",
    "# 1) Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ: Ïù¥ÎØ∏ÏßÄ-Ï°∞Ïù∏Ìä∏ Îß§Ïπ≠ Î∞è Ïù∏Îç±Ïã±\n",
    "# =========================================================\n",
    "\n",
    "IMG_RE = re.compile(r\"zed_(?P<serial>\\d+)_(?P<view>[a-zA-Z]+)_(?P<ts>\\d+\\.\\d+)\\.jpg$\")\n",
    "JNT_RE = re.compile(r\"joint_(?P<robotserial>\\d+)_(?P<ts>\\d+\\.\\d+)\\.json$\")\n",
    "\n",
    "def parse_img_fname(path: str) -> Optional[Dict[str, Any]]:\n",
    "    m = IMG_RE.search(os.path.basename(path))\n",
    "    if not m:\n",
    "        return None\n",
    "    d = m.groupdict()\n",
    "    d[\"timestamp\"] = float(d.pop(\"ts\"))\n",
    "    d[\"path\"] = path\n",
    "    return d\n",
    "\n",
    "def parse_joint_fname(path: str) -> Optional[Dict[str, Any]]:\n",
    "    m = JNT_RE.search(os.path.basename(path))\n",
    "    if not m:\n",
    "        return None\n",
    "    d = m.groupdict()\n",
    "    d[\"timestamp\"] = float(d.pop(\"ts\"))\n",
    "    d[\"path\"] = path\n",
    "    return d\n",
    "\n",
    "def flatten_json(prefix: str, obj: Any) -> Dict[str, Any]:\n",
    "    out = {}\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            out.update(flatten_json(f\"{prefix}.{k}\" if prefix else str(k), v))\n",
    "    elif isinstance(obj, list):\n",
    "        for i, v in enumerate(obj):\n",
    "            out.update(flatten_json(f\"{prefix}.{i}\" if prefix else str(i), v))\n",
    "    else:\n",
    "        out[prefix] = obj\n",
    "    return out\n",
    "\n",
    "def load_joint_angles(path: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        return {\"joint_load_error\": str(e)}\n",
    "    candidates = [\"joint_angles\", \"joints\", \"angles\", \"q\", \"positions\", \"joint\"]\n",
    "    for key in candidates:\n",
    "        if isinstance(data, dict) and key in data:\n",
    "            return flatten_json(\"joint\", data[key])\n",
    "    return flatten_json(\"joint\", data)\n",
    "\n",
    "def build_joint_index(joint_dir: str) -> Tuple[List[float], List[str]]:\n",
    "    jpaths = sorted(glob.glob(os.path.join(joint_dir, \"joint_*.json\")))\n",
    "    ts_list, path_list = [], []\n",
    "    for p in jpaths:\n",
    "        info = parse_joint_fname(p)\n",
    "        if info is None:\n",
    "            continue\n",
    "        ts_list.append(info[\"timestamp\"])\n",
    "        path_list.append(info[\"path\"])\n",
    "    pairs = sorted(zip(ts_list, path_list), key=lambda x: x[0])\n",
    "    return [t for t, _ in pairs], [p for _, p in pairs]\n",
    "\n",
    "def find_nearest_joint_any(ts: float, ts_index: List[float], paths: List[str]):\n",
    "    if not ts_index:\n",
    "        return None\n",
    "    pos = bisect_left(ts_index, ts)\n",
    "    best = None\n",
    "    for idx in (pos - 1, pos):\n",
    "        if 0 <= idx < len(ts_index):\n",
    "            jt = ts_index[idx]\n",
    "            dt = abs(jt - ts)\n",
    "            cand = (jt, paths[idx], dt)\n",
    "            if (best is None) or (dt < best[2]):\n",
    "                best = cand\n",
    "    return best\n",
    "\n",
    "def scan_images(img_dirs: List[str]) -> List[Dict[str, Any]]:\n",
    "    imgs = []\n",
    "    for d in img_dirs:\n",
    "        for p in sorted(glob.glob(os.path.join(d, \"*.jpg\"))):\n",
    "            info = parse_img_fname(p)\n",
    "            if info:\n",
    "                imgs.append(info)\n",
    "    return imgs\n",
    "\n",
    "def process_dataset_indexing(dataset_root: str, max_time_diff: float = 0.2):\n",
    "    img_dirs = [os.path.join(dataset_root, x) for x in (\"left\", \"right\", \"top\")]\n",
    "    joint_dir = os.path.join(dataset_root, \"joint\")\n",
    "    out_csv = os.path.join(dataset_root, \"matched_index.csv\")\n",
    "    out_jsonl = os.path.join(dataset_root, \"matched_index.jsonl\")\n",
    "\n",
    "    joint_ts_index, joint_paths = build_joint_index(joint_dir)\n",
    "    images = scan_images(img_dirs)\n",
    "\n",
    "    if not images:\n",
    "        print(f\"[{os.path.basename(dataset_root)}] Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "        return\n",
    "    if not joint_ts_index:\n",
    "        print(f\"[{os.path.basename(dataset_root)}] Ï°∞Ïù∏Ìä∏(JSON)Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "        return\n",
    "\n",
    "    records = []\n",
    "    unmatched_too_far = 0\n",
    "    unmatched_no_joint = 0\n",
    "\n",
    "    for img in images:\n",
    "        ts_img = img[\"timestamp\"]\n",
    "        nearest = find_nearest_joint_any(ts_img, joint_ts_index, joint_paths)\n",
    "\n",
    "        if nearest is None:\n",
    "            print(f\"[{os.path.basename(dataset_root)}] UNMATCHED(no_joint): {img['path']}\")\n",
    "            unmatched_no_joint += 1\n",
    "            continue\n",
    "\n",
    "        joint_ts, joint_path, dt = nearest\n",
    "\n",
    "        if dt > max_time_diff:\n",
    "            print(\n",
    "                f\"[{os.path.basename(dataset_root)}] UNMATCHED(threshold) \"\n",
    "                f\"dt={dt:.9f}s > {max_time_diff:.9f}s | img_ts={ts_img:.9f} \"\n",
    "                f\"-> nearest_joint={os.path.basename(joint_path)} (joint_ts={joint_ts:.9f})\"\n",
    "            )\n",
    "            unmatched_too_far += 1\n",
    "            continue\n",
    "\n",
    "        joint_cols = load_joint_angles(joint_path)\n",
    "        rec = {\n",
    "            \"img.path\": img[\"path\"],\n",
    "            \"img.serial\": img[\"serial\"],\n",
    "            \"img.view\": img[\"view\"],\n",
    "            \"img.ts\": ts_img,\n",
    "            \"joint.path\": joint_path,\n",
    "            \"joint.ts\": joint_ts,\n",
    "            \"abs_dt\": dt\n",
    "        }\n",
    "        rec.update(joint_cols)\n",
    "        records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        print(\n",
    "            f\"[{os.path.basename(dataset_root)}] Îß§Ïπ≠Îêú ÏåçÏù¥ ÏóÜÏùå \"\n",
    "            f\"(threshold={max_time_diff}s, images={len(images)}, \"\n",
    "            f\"too_far={unmatched_too_far}, no_joint={unmatched_no_joint})\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(records).sort_values(by=[\"img.ts\"]).reset_index(drop=True)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in df.iterrows():\n",
    "            f.write(json.dumps(row.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(\n",
    "        f\"[{os.path.basename(dataset_root)}] ÏôÑÎ£å: \"\n",
    "        f\"matched={len(records)} / images={len(images)} \"\n",
    "        f\"(too_far={unmatched_too_far}, no_joint={unmatched_no_joint}) \"\n",
    "        f\"-> {out_csv}, {out_jsonl}\"\n",
    "    )\n",
    "\n",
    "# =========================================================\n",
    "# 2) SegFormer Î™®Îç∏ Ï†ïÏùò\n",
    "# =========================================================\n",
    "class SegFormerForRobotArm(nn.Module):\n",
    "    def __init__(self, num_classes=2, model_name=\"nvidia/mit-b2\"):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        print(f\"üèóÔ∏è SegFormer Î™®Îç∏ Î°úÎî©: {model_name}\")\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        total_params = sum(p.numel() for p in self.segformer.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.segformer.parameters() if p.requires_grad)\n",
    "        print(f\"üìä Ï¥ù ÌååÎùºÎØ∏ÌÑ∞: {total_params:,}\")\n",
    "        print(f\"üìä ÌõàÎ†® Í∞ÄÎä• ÌååÎùºÎØ∏ÌÑ∞: {trainable_params:,}\")\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.segformer(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits,\n",
    "            size=pixel_values.shape[-2:],\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        return upsampled_logits\n",
    "\n",
    "# =========================================================\n",
    "# 3) Dataset\n",
    "# =========================================================\n",
    "def mask_to_bbox(mask: np.ndarray, min_area: int = 100) -> Optional[Tuple[int, int, int, int]]:\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    if len(xs) == 0:\n",
    "        return None\n",
    "    x1, x2 = int(xs.min()), int(xs.max())\n",
    "    y1, y2 = int(ys.min()), int(ys.max())\n",
    "    if (x2 - x1 + 1) * (y2 - y1 + 1) < min_area:\n",
    "        return None\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def crop_with_padding(img: np.ndarray, box: Tuple[int, int, int, int], pad: int = 10) -> np.ndarray:\n",
    "    H, W = img.shape[:2]\n",
    "    x1, y1, x2, y2 = box\n",
    "    x1 = max(0, x1 - pad)\n",
    "    y1 = max(0, y1 - pad)\n",
    "    x2 = min(W - 1, x2 + pad)\n",
    "    y2 = min(H - 1, y2 + pad)\n",
    "    return img[y1:y2+1, x1:x2+1]\n",
    "\n",
    "def dh_transform(a: float, alpha: float, d: float, theta: float) -> np.ndarray:\n",
    "    ca, sa = math.cos(alpha), math.sin(alpha)\n",
    "    ct, st = math.cos(theta), math.sin(theta)\n",
    "    return np.array([\n",
    "        [ct, -st*ca,  st*sa, a*ct],\n",
    "        [st,  ct*ca, -ct*sa, a*st],\n",
    "        [0.0,    sa,    ca,     d],\n",
    "        [0.0,  0.0,   0.0,   1.0]\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "def angle_to_joint_coordinate(joint_angles_deg: List[float]) -> np.ndarray:\n",
    "    fr5_dh_parameters = [\n",
    "        {'alpha':   90, 'a':  0.0, 'd': 0.152, 'theta_offset': 0},\n",
    "        {'alpha':    0, 'a': -0.425, 'd': 0.0, 'theta_offset': 0},\n",
    "        {'alpha':    0, 'a': -0.395, 'd': 0.0, 'theta_offset': 0},\n",
    "        {'alpha':   90, 'a':  0.0, 'd': 0.102, 'theta_offset': 0},\n",
    "        {'alpha':  -90, 'a':  0.0, 'd': 0.102, 'theta_offset': 0},\n",
    "        {'alpha':    0, 'a':  0.0, 'd': 0.100, 'theta_offset': 0},\n",
    "    ]\n",
    "    assert len(joint_angles_deg) == 6, \"joint_angles_deg must have length 6.\"\n",
    "    T_cum = np.eye(4, dtype=np.float64)\n",
    "    joints_xyz = [T_cum[:3, 3].copy()]\n",
    "    for i in range(len(fr5_dh_parameters)):\n",
    "        p = fr5_dh_parameters[i]\n",
    "        alpha = math.radians(p['alpha'])\n",
    "        theta = math.radians(joint_angles_deg[i] + p['theta_offset'])\n",
    "        a, d = p['a'], p['d']\n",
    "        A_i = dh_transform(a, alpha, d, theta)\n",
    "        T_cum = T_cum @ A_i\n",
    "        joints_xyz.append(T_cum[:3, 3].copy())\n",
    "    joints_xyz = np.stack(joints_xyz, axis=0)\n",
    "    return joints_xyz\n",
    "\n",
    "def maybe_to_degrees(joint_angles: List[float]) -> List[float]:\n",
    "    if all(abs(a) <= math.pi * 1.25 for a in joint_angles):\n",
    "        return [math.degrees(a) for a in joint_angles]\n",
    "    return joint_angles\n",
    "\n",
    "def extract_joint_angles_from_row(row: pd.Series) -> Optional[List[float]]:\n",
    "    keys_idx = [f\"joint.{i}\" for i in range(6)]\n",
    "    if all(k in row for k in keys_idx):\n",
    "        vals = [float(row[k]) for k in keys_idx]\n",
    "        return maybe_to_degrees(vals)\n",
    "    keys_q = [f\"joint.q{i+1}\" for i in range(6)]\n",
    "    if all(k in row for k in keys_q):\n",
    "        vals = [float(row[k]) for k in keys_q]\n",
    "        return maybe_to_degrees(vals)\n",
    "    joint_cols = [k for k in row.index if isinstance(k, str) and k.startswith(\"joint.\")]\n",
    "    nums = []\n",
    "    for k in sorted(joint_cols):\n",
    "        v = row[k]\n",
    "        try:\n",
    "            v = float(v)\n",
    "            nums.append(v)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if len(nums) >= 6:\n",
    "        return maybe_to_degrees(nums[:6])\n",
    "    return None\n",
    "\n",
    "class RobotArmSegFKDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_paths: List[str],\n",
    "        segformer_model_path: str,  # Change this to accept the path, not the model object\n",
    "        device: torch.device,\n",
    "        view_filter: Optional[str] = None,\n",
    "        image_size: int = 384,\n",
    "        run_segmentation: bool = True,\n",
    "        fg_class_id: int = 1,\n",
    "        threshold: float = 0.5,\n",
    "        pad: int = 10,\n",
    "        img_key: str = \"img.path\",\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.run_seg = run_segmentation\n",
    "        self.fg_class_id = fg_class_id\n",
    "        self.threshold = threshold\n",
    "        self.image_size = image_size\n",
    "        self.pad = pad\n",
    "        self.img_key = img_key\n",
    "\n",
    "        rows = []\n",
    "        for p in index_paths:\n",
    "            if p.endswith(\".csv\"):\n",
    "                df = pd.read_csv(p)\n",
    "            elif p.endswith(\".jsonl\"):\n",
    "                df = pd.read_json(p, lines=True)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported index file: {p}\")\n",
    "            rows.append(df)\n",
    "        self.df = pd.concat(rows, ignore_index=True)\n",
    "        if view_filter is not None:\n",
    "            self.df = self.df[self.df.get(\"img.view\", \"\") == view_filter].reset_index(drop=True)\n",
    "        \n",
    "        # --- NEW CODE: Load SegFormer model inside the __init__ method ---\n",
    "        if self.run_seg:\n",
    "            self.model = SegFormerForRobotArm(num_classes=2, model_name=\"nvidia/mit-b2\").to(device)\n",
    "            checkpoint = torch.load(segformer_model_path, map_location=self.device)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.model.eval()\n",
    "        else:\n",
    "            self.model = None\n",
    "\n",
    "        # AlbumentationsÎ•º ÏÇ¨Ïö©Ìïú Î≥ÄÌôò ÌååÏù¥ÌîÑÎùºÏù∏\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(image_size, image_size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "        self.seg_input_transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _infer_mask(self, pil_img: Image.Image) -> np.ndarray:\n",
    "        inp = self.seg_input_transform(pil_img).unsqueeze(0).to(self.device)\n",
    "        self.model.eval()\n",
    "        logits = self.model(inp)\n",
    "        if isinstance(logits, (list, tuple)):\n",
    "            logits = logits[0]\n",
    "        probs = torch.softmax(logits, dim=1)[0]\n",
    "        fg_prob = probs[self.fg_class_id]\n",
    "        mask = (fg_prob > self.threshold).float().cpu().numpy()\n",
    "        return (mask > 0.5).astype(np.uint8)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row[self.img_key]\n",
    "        bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.run_seg:\n",
    "            pil = Image.fromarray(rgb)\n",
    "            mask = self._infer_mask(pil)\n",
    "            box = mask_to_bbox(mask, min_area=100)\n",
    "        else:\n",
    "            mask, box = None, None\n",
    "\n",
    "        if box is None:\n",
    "            H, W = rgb.shape[:2]\n",
    "            side = min(H, W)\n",
    "            cy, cx = H // 2, W // 2\n",
    "            x1 = max(0, cx - side // 2)\n",
    "            y1 = max(0, cy - side // 2)\n",
    "            x2 = min(W - 1, x1 + side - 1)\n",
    "            y2 = min(H - 1, y1 + side - 1)\n",
    "            crop = rgb[y1:y2+1, x1:x2+1]\n",
    "        else:\n",
    "            crop = crop_with_padding(rgb, box, pad=self.pad)\n",
    "\n",
    "        transformed = self.transform(image=crop)\n",
    "        img_tensor = transformed['image']\n",
    "\n",
    "        joint_angles = extract_joint_angles_from_row(row)\n",
    "        if joint_angles is None:\n",
    "            raise ValueError(f\"No valid joint angles in row index={idx} (columns like 'joint.*' expected).\")\n",
    "\n",
    "        joints_xyz = angle_to_joint_coordinate(joint_angles)\n",
    "        joints_tensor = torch.from_numpy(joints_xyz).float()\n",
    "\n",
    "        sample = {\n",
    "            \"image\": img_tensor,\n",
    "            \"joints_xyz\": joints_tensor,\n",
    "            \"img_path\": img_path,\n",
    "            \"view\": row.get(\"img.view\", None),\n",
    "            \"img_ts\": float(row.get(\"img.ts\", -1)),\n",
    "            \"joint_ts\": float(row.get(\"joint.ts\", -1)),\n",
    "        }\n",
    "        if box is not None:\n",
    "            sample[\"roi_box_xyxy\"] = np.array(box, dtype=np.int32)\n",
    "        return sample\n",
    "    \n",
    "# =========================================================\n",
    "# 4) Model (HRNet-like with ViT backbone, 7 keypoints)\n",
    "# =========================================================\n",
    "def reshape_vit_output(x, patch_size=16, img_size=512):\n",
    "    if x.ndim == 3 and x.shape[1] > 1:\n",
    "        if x.shape[1] == (img_size // patch_size)**2 + 1:\n",
    "            x = x[:, 1:]\n",
    "        B, N, D = x.shape\n",
    "        H_feat = W_feat = int(N**0.5)\n",
    "        if H_feat * W_feat != N:\n",
    "            raise ValueError(f\"ViT output sequence length {N} is not a perfect square.\")\n",
    "        x = x.permute(0, 2, 1).reshape(B, D, H_feat, W_feat)\n",
    "    elif x.ndim == 4:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported ViT output shape for reshaping: {x.shape}\")\n",
    "    return x\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.down = (nn.Conv2d(in_c, out_c, 1, stride, bias=False)\n",
    "                     if (stride != 1 or in_c != out_c) else None)\n",
    "    def forward(self, x):\n",
    "        res = self.down(x) if self.down else x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class PoseEstimationHRViT(nn.Module):\n",
    "    def __init__(self, num_kp=7, vit_model_name='vit_base_patch16_224', pretrained=True, img_size=512):\n",
    "        super().__init__()\n",
    "        self.num_kp = num_kp\n",
    "        self.img_size = img_size\n",
    "        self.vit_backbone = timm.create_model(vit_model_name, pretrained=pretrained, num_classes=0)\n",
    "        print(f\"Loaded pretrained ViT model: {vit_model_name}\")\n",
    "        vit_embed_dim = self.vit_backbone.embed_dim\n",
    "        vit_patch_size = self.vit_backbone.patch_embed.patch_size[0]\n",
    "        \n",
    "        self.high_res_branch_conv = nn.Sequential(\n",
    "            nn.Conv2d(vit_embed_dim, 256, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            self._make_layer(256, 256, 2, 1)\n",
    "        )\n",
    "        self.low_res_branch_conv = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            self._make_layer(512, 512, 2, 1)\n",
    "        )\n",
    "        self.fuse_l_to_h = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        )\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_kp * 3)\n",
    "        )\n",
    "    def _make_layer(self, in_c, out_c, blocks, stride):\n",
    "        layers = [BasicBlock(in_c, out_c, stride)]\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(out_c, out_c))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        vit_output = self.vit_backbone.forward_features(x)\n",
    "        vit_spatial_features = reshape_vit_output(vit_output, self.vit_backbone.patch_embed.patch_size[0], self.img_size)\n",
    "        high_res_feat = self.high_res_branch_conv(vit_spatial_features)\n",
    "        low_res_feat = self.low_res_branch_conv(high_res_feat)\n",
    "        high_res_fused = high_res_feat + self.fuse_l_to_h(low_res_feat)\n",
    "        output = self.regression_head(high_res_fused)\n",
    "        return output.view(-1, self.num_kp, 3)\n",
    "\n",
    "# =========================================================\n",
    "# 5) ÌïôÏäµ Î∞è Í≤ÄÏ¶ù Î£®ÌîÑ\n",
    "# =========================================================\n",
    "def train_and_validate(\n",
    "    model: nn.Module,\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    save_path: str,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=os.cpu_count() // 2,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=os.cpu_count() // 2,\n",
    "    )\n",
    "    best_val_loss = float('inf')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        for batch in train_progress_bar:\n",
    "            images = batch['image'].to(device)\n",
    "            joints_xyz = batch['joints_xyz'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, joints_xyz)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            train_progress_bar.set_postfix(loss=loss.item())\n",
    "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_progress_bar:\n",
    "                images = batch['image'].to(device)\n",
    "                joints_xyz = batch['joints_xyz'].to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, joints_xyz)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                val_progress_bar.set_postfix(loss=loss.item())\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            print(f\"Validation loss improved from {best_val_loss:.4f} to {epoch_val_loss:.4f}. Saving model...\")\n",
    "            best_val_loss = epoch_val_loss\n",
    "            checkpoint_path = os.path.join(save_path, \"best_model_checkpoint.pt\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "# =========================================================\n",
    "# 6) Ïã§Ìñâ Î©îÏù∏ Ìï®Ïàò\n",
    "# =========================================================\n",
    "def main():\n",
    "    # Îç∞Ïù¥ÌÑ∞ÏÖã Ïù∏Îç±Ïã± (Ìïú Î≤àÎßå Ïã§ÌñâÌïòÎ©¥ Îê©ÎãàÎã§)\n",
    "    subdirs = [\n",
    "        os.path.join(ROOT, d) for d in os.listdir(ROOT)\n",
    "        if os.path.isdir(os.path.join(ROOT, d)) and d.startswith(\"Fr5_intertek_\")\n",
    "    ]\n",
    "    subdirs.sort()\n",
    "    print(f\"Ï¥ù {len(subdirs)}Í∞ú Îç∞Ïù¥ÌÑ∞ÏÖã ÏÑ∏Ìä∏ ÌÉêÏÉâ: {subdirs}\")\n",
    "    for sd in subdirs:\n",
    "        process_dataset_indexing(sd, MAX_TIME_DIFF)\n",
    "\n",
    "    # SegFormer Î™®Îç∏ Í≤ΩÎ°úÎßå Ï†ÄÏû•\n",
    "    SegFormer_model_path = \"/home/najo/NAS/DIP/Fr5_robot_SegFormer/best_segformer_robot_arm.pth\"\n",
    "    \n",
    "    train_index_files = [os.path.join(ROOT, 'Fr5_intertek_1st_250526/matched_index.csv'),\n",
    "                         os.path.join(ROOT, 'Fr5_intertek_2nd_250526/matched_index.csv'),\n",
    "                         os.path.join(ROOT, 'Fr5_intertek_3rd_250526/matched_index.csv'),\n",
    "                         os.path.join(ROOT, 'Fr5_intertek_4th_250526/matched_index.csv'),\n",
    "                         os.path.join(ROOT, 'Fr5_intertek_5th_250526/matched_index.csv'),\n",
    "                         os.path.join(ROOT, 'Fr5_intertek_6th_250526/matched_index.csv'),\n",
    "                         ]\n",
    "    val_index_files = [os.path.join(ROOT, 'Fr5_intertek_7th_250526/matched_index.csv')]\n",
    "\n",
    "    IMG_SIZE = 512\n",
    "    \n",
    "    train_dataset = RobotArmSegFKDataset(\n",
    "        index_paths=train_index_files,\n",
    "        segformer_model_path=SegFormer_model_path, # Pass the path\n",
    "        device=device,\n",
    "        image_size=IMG_SIZE,\n",
    "        run_segmentation=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = RobotArmSegFKDataset(\n",
    "        index_paths=val_index_files,\n",
    "        segformer_model_path=SegFormer_model_path, # Pass the path\n",
    "        device=device,\n",
    "        image_size=IMG_SIZE,\n",
    "        run_segmentation=True\n",
    "    )\n",
    "    \n",
    "    model = PoseEstimationHRViT(num_kp=7, pretrained=True, img_size=IMG_SIZE).to(device)\n",
    "\n",
    "    train_and_validate(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        epochs=50,\n",
    "        batch_size=4,\n",
    "        learning_rate=1e-4,\n",
    "        save_path=\"checkpoints\",\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.multiprocessing.set_ start_method('spawn', force=True)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8069281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc0ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae285cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
