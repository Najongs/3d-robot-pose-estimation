{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8437d0",
   "metadata": {},
   "source": [
    "### FR5 로봇 데이터셋 전처리 과정\n",
    "1. 카메라와 Joint angle의 Time stamps 맞추기\n",
    "2. ROI에 맞게 적절한 label 생성 \n",
    "\n",
    "### FR5 로봇 자세 추정 과정\n",
    "1. 전체 이미지에서 ROI 탐색 - SegFormer\n",
    "2. ROI 이미지에서 각 관절 좌표 추정 3차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659a55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "총 7개 데이터셋 세트 탐색: ['/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_1st_250526', '/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_2nd_250526', '/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_3rd_250526', '/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_4th_250526', '/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_5th_250526', '/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_6th_250526', '/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_7th_250526']\n",
      "[Fr5_intertek_1st_250526] 완료: matched=1296 / images=1296 (too_far=0, no_joint=0) -> /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_1st_250526/matched_index.csv, /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_1st_250526/matched_index.jsonl\n",
      "[Fr5_intertek_2nd_250526] 완료: matched=1310 / images=1310 (too_far=0, no_joint=0) -> /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_2nd_250526/matched_index.csv, /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_2nd_250526/matched_index.jsonl\n",
      "[Fr5_intertek_3rd_250526] 완료: matched=1308 / images=1308 (too_far=0, no_joint=0) -> /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_3rd_250526/matched_index.csv, /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_3rd_250526/matched_index.jsonl\n",
      "[Fr5_intertek_4th_250526] 완료: matched=1310 / images=1310 (too_far=0, no_joint=0) -> /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_4th_250526/matched_index.csv, /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_4th_250526/matched_index.jsonl\n",
      "[Fr5_intertek_5th_250526] 완료: matched=1314 / images=1314 (too_far=0, no_joint=0) -> /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_5th_250526/matched_index.csv, /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_5th_250526/matched_index.jsonl\n",
      "[Fr5_intertek_6th_250526] 완료: matched=1296 / images=1296 (too_far=0, no_joint=0) -> /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_6th_250526/matched_index.csv, /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_6th_250526/matched_index.jsonl\n",
      "[Fr5_intertek_7th_250526] 완료: matched=1308 / images=1308 (too_far=0, no_joint=0) -> /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_7th_250526/matched_index.csv, /home/najo/NAS/DIP/datasets/Fr5_intertek_dataset/Fr5_intertek_7th_250526/matched_index.jsonl\n",
      "🏗️ SegFormer 모델 로딩: nvidia/mit-b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b2 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_540144/3405246827.py:348: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(segformer_model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 총 파라미터: 27,348,162\n",
      "📊 훈련 가능 파라미터: 27,348,162\n",
      "🏗️ SegFormer 모델 로딩: nvidia/mit-b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b2 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_540144/3405246827.py:348: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(segformer_model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 총 파라미터: 27,348,162\n",
      "📊 훈련 가능 파라미터: 27,348,162\n",
      "Loaded pretrained ViT model: vit_base_patch16_224\n",
      "Starting training for 50 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]:   0%|          | 0/1959 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/najo/.conda/envs/dip/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/najo/.conda/envs/dip/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'RobotArmSegFKDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from bisect import bisect_left\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig\n",
    "import timm\n",
    "\n",
    "# ================= MKL 충돌 해결 및 장치 설정 =================\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================= 사용자 설정 =================\n",
    "ROOT = \"/home/najo/NAS/DIP/datasets/Fr5_intertek_dataset\"\n",
    "MAX_TIME_DIFF = 0.025  # seconds\n",
    "# ==============================================\n",
    "\n",
    "# =========================================================\n",
    "# 1) 데이터 준비: 이미지-조인트 매칭 및 인덱싱\n",
    "# =========================================================\n",
    "\n",
    "IMG_RE = re.compile(r\"zed_(?P<serial>\\d+)_(?P<view>[a-zA-Z]+)_(?P<ts>\\d+\\.\\d+)\\.jpg$\")\n",
    "JNT_RE = re.compile(r\"joint_(?P<robotserial>\\d+)_(?P<ts>\\d+\\.\\d+)\\.json$\")\n",
    "\n",
    "def parse_img_fname(path: str) -> Optional[Dict[str, Any]]:\n",
    "    m = IMG_RE.search(os.path.basename(path))\n",
    "    if not m:\n",
    "        return None\n",
    "    d = m.groupdict()\n",
    "    d[\"timestamp\"] = float(d.pop(\"ts\"))\n",
    "    d[\"path\"] = path\n",
    "    return d\n",
    "\n",
    "def parse_joint_fname(path: str) -> Optional[Dict[str, Any]]:\n",
    "    m = JNT_RE.search(os.path.basename(path))\n",
    "    if not m:\n",
    "        return None\n",
    "    d = m.groupdict()\n",
    "    d[\"timestamp\"] = float(d.pop(\"ts\"))\n",
    "    d[\"path\"] = path\n",
    "    return d\n",
    "\n",
    "def flatten_json(prefix: str, obj: Any) -> Dict[str, Any]:\n",
    "    out = {}\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            out.update(flatten_json(f\"{prefix}.{k}\" if prefix else str(k), v))\n",
    "    elif isinstance(obj, list):\n",
    "        for i, v in enumerate(obj):\n",
    "            out.update(flatten_json(f\"{prefix}.{i}\" if prefix else str(i), v))\n",
    "    else:\n",
    "        out[prefix] = obj\n",
    "    return out\n",
    "\n",
    "def load_joint_angles(path: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        return {\"joint_load_error\": str(e)}\n",
    "    candidates = [\"joint_angles\", \"joints\", \"angles\", \"q\", \"positions\", \"joint\"]\n",
    "    for key in candidates:\n",
    "        if isinstance(data, dict) and key in data:\n",
    "            return flatten_json(\"joint\", data[key])\n",
    "    return flatten_json(\"joint\", data)\n",
    "\n",
    "def build_joint_index(joint_dir: str) -> Tuple[List[float], List[str]]:\n",
    "    jpaths = sorted(glob.glob(os.path.join(joint_dir, \"joint_*.json\")))\n",
    "    ts_list, path_list = [], []\n",
    "    for p in jpaths:\n",
    "        info = parse_joint_fname(p)\n",
    "        if info is None:\n",
    "            continue\n",
    "        ts_list.append(info[\"timestamp\"])\n",
    "        path_list.append(info[\"path\"])\n",
    "    pairs = sorted(zip(ts_list, path_list), key=lambda x: x[0])\n",
    "    return [t for t, _ in pairs], [p for _, p in pairs]\n",
    "\n",
    "def find_nearest_joint_any(ts: float, ts_index: List[float], paths: List[str]):\n",
    "    if not ts_index:\n",
    "        return None\n",
    "    pos = bisect_left(ts_index, ts)\n",
    "    best = None\n",
    "    for idx in (pos - 1, pos):\n",
    "        if 0 <= idx < len(ts_index):\n",
    "            jt = ts_index[idx]\n",
    "            dt = abs(jt - ts)\n",
    "            cand = (jt, paths[idx], dt)\n",
    "            if (best is None) or (dt < best[2]):\n",
    "                best = cand\n",
    "    return best\n",
    "\n",
    "def scan_images(img_dirs: List[str]) -> List[Dict[str, Any]]:\n",
    "    imgs = []\n",
    "    for d in img_dirs:\n",
    "        for p in sorted(glob.glob(os.path.join(d, \"*.jpg\"))):\n",
    "            info = parse_img_fname(p)\n",
    "            if info:\n",
    "                imgs.append(info)\n",
    "    return imgs\n",
    "\n",
    "def process_dataset_indexing(dataset_root: str, max_time_diff: float = 0.2):\n",
    "    img_dirs = [os.path.join(dataset_root, x) for x in (\"left\", \"right\", \"top\")]\n",
    "    joint_dir = os.path.join(dataset_root, \"joint\")\n",
    "    out_csv = os.path.join(dataset_root, \"matched_index.csv\")\n",
    "    out_jsonl = os.path.join(dataset_root, \"matched_index.jsonl\")\n",
    "\n",
    "    joint_ts_index, joint_paths = build_joint_index(joint_dir)\n",
    "    images = scan_images(img_dirs)\n",
    "\n",
    "    if not images:\n",
    "        print(f\"[{os.path.basename(dataset_root)}] 이미지가 없습니다.\")\n",
    "        return\n",
    "    if not joint_ts_index:\n",
    "        print(f\"[{os.path.basename(dataset_root)}] 조인트(JSON)가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    records = []\n",
    "    unmatched_too_far = 0\n",
    "    unmatched_no_joint = 0\n",
    "\n",
    "    for img in images:\n",
    "        ts_img = img[\"timestamp\"]\n",
    "        nearest = find_nearest_joint_any(ts_img, joint_ts_index, joint_paths)\n",
    "\n",
    "        if nearest is None:\n",
    "            print(f\"[{os.path.basename(dataset_root)}] UNMATCHED(no_joint): {img['path']}\")\n",
    "            unmatched_no_joint += 1\n",
    "            continue\n",
    "\n",
    "        joint_ts, joint_path, dt = nearest\n",
    "\n",
    "        if dt > max_time_diff:\n",
    "            print(\n",
    "                f\"[{os.path.basename(dataset_root)}] UNMATCHED(threshold) \"\n",
    "                f\"dt={dt:.9f}s > {max_time_diff:.9f}s | img_ts={ts_img:.9f} \"\n",
    "                f\"-> nearest_joint={os.path.basename(joint_path)} (joint_ts={joint_ts:.9f})\"\n",
    "            )\n",
    "            unmatched_too_far += 1\n",
    "            continue\n",
    "\n",
    "        joint_cols = load_joint_angles(joint_path)\n",
    "        rec = {\n",
    "            \"img.path\": img[\"path\"],\n",
    "            \"img.serial\": img[\"serial\"],\n",
    "            \"img.view\": img[\"view\"],\n",
    "            \"img.ts\": ts_img,\n",
    "            \"joint.path\": joint_path,\n",
    "            \"joint.ts\": joint_ts,\n",
    "            \"abs_dt\": dt\n",
    "        }\n",
    "        rec.update(joint_cols)\n",
    "        records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        print(\n",
    "            f\"[{os.path.basename(dataset_root)}] 매칭된 쌍이 없음 \"\n",
    "            f\"(threshold={max_time_diff}s, images={len(images)}, \"\n",
    "            f\"too_far={unmatched_too_far}, no_joint={unmatched_no_joint})\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(records).sort_values(by=[\"img.ts\"]).reset_index(drop=True)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in df.iterrows():\n",
    "            f.write(json.dumps(row.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(\n",
    "        f\"[{os.path.basename(dataset_root)}] 완료: \"\n",
    "        f\"matched={len(records)} / images={len(images)} \"\n",
    "        f\"(too_far={unmatched_too_far}, no_joint={unmatched_no_joint}) \"\n",
    "        f\"-> {out_csv}, {out_jsonl}\"\n",
    "    )\n",
    "\n",
    "# =========================================================\n",
    "# 2) SegFormer 모델 정의\n",
    "# =========================================================\n",
    "class SegFormerForRobotArm(nn.Module):\n",
    "    def __init__(self, num_classes=2, model_name=\"nvidia/mit-b2\"):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        print(f\"🏗️ SegFormer 모델 로딩: {model_name}\")\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        total_params = sum(p.numel() for p in self.segformer.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.segformer.parameters() if p.requires_grad)\n",
    "        print(f\"📊 총 파라미터: {total_params:,}\")\n",
    "        print(f\"📊 훈련 가능 파라미터: {trainable_params:,}\")\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.segformer(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits,\n",
    "            size=pixel_values.shape[-2:],\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        return upsampled_logits\n",
    "\n",
    "# =========================================================\n",
    "# 3) Dataset\n",
    "# =========================================================\n",
    "def mask_to_bbox(mask: np.ndarray, min_area: int = 100) -> Optional[Tuple[int, int, int, int]]:\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    if len(xs) == 0:\n",
    "        return None\n",
    "    x1, x2 = int(xs.min()), int(xs.max())\n",
    "    y1, y2 = int(ys.min()), int(ys.max())\n",
    "    if (x2 - x1 + 1) * (y2 - y1 + 1) < min_area:\n",
    "        return None\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def crop_with_padding(img: np.ndarray, box: Tuple[int, int, int, int], pad: int = 10) -> np.ndarray:\n",
    "    H, W = img.shape[:2]\n",
    "    x1, y1, x2, y2 = box\n",
    "    x1 = max(0, x1 - pad)\n",
    "    y1 = max(0, y1 - pad)\n",
    "    x2 = min(W - 1, x2 + pad)\n",
    "    y2 = min(H - 1, y2 + pad)\n",
    "    return img[y1:y2+1, x1:x2+1]\n",
    "\n",
    "def dh_transform(a: float, alpha: float, d: float, theta: float) -> np.ndarray:\n",
    "    ca, sa = math.cos(alpha), math.sin(alpha)\n",
    "    ct, st = math.cos(theta), math.sin(theta)\n",
    "    return np.array([\n",
    "        [ct, -st*ca,  st*sa, a*ct],\n",
    "        [st,  ct*ca, -ct*sa, a*st],\n",
    "        [0.0,    sa,    ca,     d],\n",
    "        [0.0,  0.0,   0.0,   1.0]\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "def angle_to_joint_coordinate(joint_angles_deg: List[float]) -> np.ndarray:\n",
    "    fr5_dh_parameters = [\n",
    "        {'alpha':   90, 'a':  0.0, 'd': 0.152, 'theta_offset': 0},\n",
    "        {'alpha':    0, 'a': -0.425, 'd': 0.0, 'theta_offset': 0},\n",
    "        {'alpha':    0, 'a': -0.395, 'd': 0.0, 'theta_offset': 0},\n",
    "        {'alpha':   90, 'a':  0.0, 'd': 0.102, 'theta_offset': 0},\n",
    "        {'alpha':  -90, 'a':  0.0, 'd': 0.102, 'theta_offset': 0},\n",
    "        {'alpha':    0, 'a':  0.0, 'd': 0.100, 'theta_offset': 0},\n",
    "    ]\n",
    "    assert len(joint_angles_deg) == 6, \"joint_angles_deg must have length 6.\"\n",
    "    T_cum = np.eye(4, dtype=np.float64)\n",
    "    joints_xyz = [T_cum[:3, 3].copy()]\n",
    "    for i in range(len(fr5_dh_parameters)):\n",
    "        p = fr5_dh_parameters[i]\n",
    "        alpha = math.radians(p['alpha'])\n",
    "        theta = math.radians(joint_angles_deg[i] + p['theta_offset'])\n",
    "        a, d = p['a'], p['d']\n",
    "        A_i = dh_transform(a, alpha, d, theta)\n",
    "        T_cum = T_cum @ A_i\n",
    "        joints_xyz.append(T_cum[:3, 3].copy())\n",
    "    joints_xyz = np.stack(joints_xyz, axis=0)\n",
    "    return joints_xyz\n",
    "\n",
    "def maybe_to_degrees(joint_angles: List[float]) -> List[float]:\n",
    "    if all(abs(a) <= math.pi * 1.25 for a in joint_angles):\n",
    "        return [math.degrees(a) for a in joint_angles]\n",
    "    return joint_angles\n",
    "\n",
    "def extract_joint_angles_from_row(row: pd.Series) -> Optional[List[float]]:\n",
    "    keys_idx = [f\"joint.{i}\" for i in range(6)]\n",
    "    if all(k in row for k in keys_idx):\n",
    "        vals = [float(row[k]) for k in keys_idx]\n",
    "        return maybe_to_degrees(vals)\n",
    "    keys_q = [f\"joint.q{i+1}\" for i in range(6)]\n",
    "    if all(k in row for k in keys_q):\n",
    "        vals = [float(row[k]) for k in keys_q]\n",
    "        return maybe_to_degrees(vals)\n",
    "    joint_cols = [k for k in row.index if isinstance(k, str) and k.startswith(\"joint.\")]\n",
    "    nums = []\n",
    "    for k in sorted(joint_cols):\n",
    "        v = row[k]\n",
    "        try:\n",
    "            v = float(v)\n",
    "            nums.append(v)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if len(nums) >= 6:\n",
    "        return maybe_to_degrees(nums[:6])\n",
    "    return None\n",
    "\n",
    "class RobotArmSegFKDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_paths: List[str],\n",
    "        segformer_model_path: str,  # Change this to accept the path, not the model object\n",
    "        device: torch.device,\n",
    "        view_filter: Optional[str] = None,\n",
    "        image_size: int = 384,\n",
    "        run_segmentation: bool = True,\n",
    "        fg_class_id: int = 1,\n",
    "        threshold: float = 0.5,\n",
    "        pad: int = 10,\n",
    "        img_key: str = \"img.path\",\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.run_seg = run_segmentation\n",
    "        self.fg_class_id = fg_class_id\n",
    "        self.threshold = threshold\n",
    "        self.image_size = image_size\n",
    "        self.pad = pad\n",
    "        self.img_key = img_key\n",
    "\n",
    "        rows = []\n",
    "        for p in index_paths:\n",
    "            if p.endswith(\".csv\"):\n",
    "                df = pd.read_csv(p)\n",
    "            elif p.endswith(\".jsonl\"):\n",
    "                df = pd.read_json(p, lines=True)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported index file: {p}\")\n",
    "            rows.append(df)\n",
    "        self.df = pd.concat(rows, ignore_index=True)\n",
    "        if view_filter is not None:\n",
    "            self.df = self.df[self.df.get(\"img.view\", \"\") == view_filter].reset_index(drop=True)\n",
    "        \n",
    "        # --- NEW CODE: Load SegFormer model inside the __init__ method ---\n",
    "        if self.run_seg:\n",
    "            self.model = SegFormerForRobotArm(num_classes=2, model_name=\"nvidia/mit-b2\").to(device)\n",
    "            checkpoint = torch.load(segformer_model_path, map_location=self.device)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.model.eval()\n",
    "        else:\n",
    "            self.model = None\n",
    "\n",
    "        # Albumentations를 사용한 변환 파이프라인\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(image_size, image_size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "        self.seg_input_transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _infer_mask(self, pil_img: Image.Image) -> np.ndarray:\n",
    "        inp = self.seg_input_transform(pil_img).unsqueeze(0).to(self.device)\n",
    "        self.model.eval()\n",
    "        logits = self.model(inp)\n",
    "        if isinstance(logits, (list, tuple)):\n",
    "            logits = logits[0]\n",
    "        probs = torch.softmax(logits, dim=1)[0]\n",
    "        fg_prob = probs[self.fg_class_id]\n",
    "        mask = (fg_prob > self.threshold).float().cpu().numpy()\n",
    "        return (mask > 0.5).astype(np.uint8)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row[self.img_key]\n",
    "        bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.run_seg:\n",
    "            pil = Image.fromarray(rgb)\n",
    "            mask = self._infer_mask(pil)\n",
    "            box = mask_to_bbox(mask, min_area=100)\n",
    "        else:\n",
    "            mask, box = None, None\n",
    "\n",
    "        if box is None:\n",
    "            H, W = rgb.shape[:2]\n",
    "            side = min(H, W)\n",
    "            cy, cx = H // 2, W // 2\n",
    "            x1 = max(0, cx - side // 2)\n",
    "            y1 = max(0, cy - side // 2)\n",
    "            x2 = min(W - 1, x1 + side - 1)\n",
    "            y2 = min(H - 1, y1 + side - 1)\n",
    "            crop = rgb[y1:y2+1, x1:x2+1]\n",
    "        else:\n",
    "            crop = crop_with_padding(rgb, box, pad=self.pad)\n",
    "\n",
    "        transformed = self.transform(image=crop)\n",
    "        img_tensor = transformed['image']\n",
    "\n",
    "        joint_angles = extract_joint_angles_from_row(row)\n",
    "        if joint_angles is None:\n",
    "            raise ValueError(f\"No valid joint angles in row index={idx} (columns like 'joint.*' expected).\")\n",
    "\n",
    "        joints_xyz = angle_to_joint_coordinate(joint_angles)\n",
    "        joints_tensor = torch.from_numpy(joints_xyz).float()\n",
    "\n",
    "        sample = {\n",
    "            \"image\": img_tensor,\n",
    "            \"joints_xyz\": joints_tensor,\n",
    "            \"img_path\": img_path,\n",
    "            \"view\": row.get(\"img.view\", None),\n",
    "            \"img_ts\": float(row.get(\"img.ts\", -1)),\n",
    "            \"joint_ts\": float(row.get(\"joint.ts\", -1)),\n",
    "        }\n",
    "        if box is not None:\n",
    "            sample[\"roi_box_xyxy\"] = np.array(box, dtype=np.int32)\n",
    "        return sample\n",
    "    \n",
    "# =========================================================\n",
    "# 4) Model (HRNet-like with ViT backbone, 7 keypoints)\n",
    "# =========================================================\n",
    "def reshape_vit_output(x, patch_size=16, img_size=512):\n",
    "    if x.ndim == 3 and x.shape[1] > 1:\n",
    "        if x.shape[1] == (img_size // patch_size)**2 + 1:\n",
    "            x = x[:, 1:]\n",
    "        B, N, D = x.shape\n",
    "        H_feat = W_feat = int(N**0.5)\n",
    "        if H_feat * W_feat != N:\n",
    "            raise ValueError(f\"ViT output sequence length {N} is not a perfect square.\")\n",
    "        x = x.permute(0, 2, 1).reshape(B, D, H_feat, W_feat)\n",
    "    elif x.ndim == 4:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported ViT output shape for reshaping: {x.shape}\")\n",
    "    return x\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.down = (nn.Conv2d(in_c, out_c, 1, stride, bias=False)\n",
    "                     if (stride != 1 or in_c != out_c) else None)\n",
    "    def forward(self, x):\n",
    "        res = self.down(x) if self.down else x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class PoseEstimationHRViT(nn.Module):\n",
    "    def __init__(self, num_kp=7, vit_model_name='vit_base_patch16_224', pretrained=True, img_size=512):\n",
    "        super().__init__()\n",
    "        self.num_kp = num_kp\n",
    "        self.img_size = img_size\n",
    "        self.vit_backbone = timm.create_model(vit_model_name, pretrained=pretrained, num_classes=0)\n",
    "        print(f\"Loaded pretrained ViT model: {vit_model_name}\")\n",
    "        vit_embed_dim = self.vit_backbone.embed_dim\n",
    "        vit_patch_size = self.vit_backbone.patch_embed.patch_size[0]\n",
    "        \n",
    "        self.high_res_branch_conv = nn.Sequential(\n",
    "            nn.Conv2d(vit_embed_dim, 256, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            self._make_layer(256, 256, 2, 1)\n",
    "        )\n",
    "        self.low_res_branch_conv = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            self._make_layer(512, 512, 2, 1)\n",
    "        )\n",
    "        self.fuse_l_to_h = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        )\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_kp * 3)\n",
    "        )\n",
    "    def _make_layer(self, in_c, out_c, blocks, stride):\n",
    "        layers = [BasicBlock(in_c, out_c, stride)]\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(out_c, out_c))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        vit_output = self.vit_backbone.forward_features(x)\n",
    "        vit_spatial_features = reshape_vit_output(vit_output, self.vit_backbone.patch_embed.patch_size[0], self.img_size)\n",
    "        high_res_feat = self.high_res_branch_conv(vit_spatial_features)\n",
    "        low_res_feat = self.low_res_branch_conv(high_res_feat)\n",
    "        high_res_fused = high_res_feat + self.fuse_l_to_h(low_res_feat)\n",
    "        output = self.regression_head(high_res_fused)\n",
    "        return output.view(-1, self.num_kp, 3)\n",
    "\n",
    "# =========================================================\n",
    "# 5) 학습 및 검증 루프\n",
    "# =========================================================\n",
    "def train_and_validate(\n",
    "    model: nn.Module,\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    save_path: str,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=os.cpu_count() // 2,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=os.cpu_count() // 2,\n",
    "    )\n",
    "    best_val_loss = float('inf')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        for batch in train_progress_bar:\n",
    "            images = batch['image'].to(device)\n",
    "            joints_xyz = batch['joints_xyz'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, joints_xyz)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            train_progress_bar.set_postfix(loss=loss.item())\n",
    "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_progress_bar:\n",
    "                images = batch['image'].to(device)\n",
    "                joints_xyz = batch['joints_xyz'].to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, joints_xyz)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                val_progress_bar.set_postfix(loss=loss.item())\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            print(f\"Validation loss improved from {best_val_loss:.4f} to {epoch_val_loss:.4f}. Saving model...\")\n",
    "            best_val_loss = epoch_val_loss\n",
    "            checkpoint_path = os.path.join(save_path, \"best_model_checkpoint.pt\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "# =========================================================\n",
    "# 6) 실행 메인 함수\n",
    "# =========================================================\n",
    "def main():\n",
    "    # 데이터셋 인덱싱 (한 번만 실행하면 됩니다)\n",
    "    subdirs = [\n",
    "        os.path.join(ROOT, d) for d in os.listdir(ROOT)\n",
    "        if os.path.isdir(os.path.join(ROOT, d)) and d.startswith(\"Fr5_intertek_\")\n",
    "    ]\n",
    "    subdirs.sort()\n",
    "    print(f\"총 {len(subdirs)}개 데이터셋 세트 탐색: {subdirs}\")\n",
    "    for sd in subdirs:\n",
    "        process_dataset_indexing(sd, MAX_TIME_DIFF)\n",
    "\n",
    "    # SegFormer 모델 경로만 저장\n",
    "    SegFormer_model_path = \"/home/najo/NAS/DIP/Fr5_robot_SegFormer/best_segformer_robot_arm.pth\"\n",
    "    \n",
    "    train_index_files = [os.path.join(ROOT, 'Fr5_intertek_1st_250526/matched_index.csv'),\n",
    "                         os.path.join(ROOT, 'Fr5_intertek_2nd_250526/matched_index.csv'),\n",
    "                         os.path.join(ROOT, 'Fr5_intertek_3rd_250526/matched_index.csv'),\n",
    "                         os.path.join(ROOT, 'Fr5_intertek_4th_250526/matched_index.csv'),\n",
    "                         os.path.join(ROOT, 'Fr5_intertek_5th_250526/matched_index.csv'),\n",
    "                         os.path.join(ROOT, 'Fr5_intertek_6th_250526/matched_index.csv'),\n",
    "                         ]\n",
    "    val_index_files = [os.path.join(ROOT, 'Fr5_intertek_7th_250526/matched_index.csv')]\n",
    "\n",
    "    IMG_SIZE = 512\n",
    "    \n",
    "    train_dataset = RobotArmSegFKDataset(\n",
    "        index_paths=train_index_files,\n",
    "        segformer_model_path=SegFormer_model_path, # Pass the path\n",
    "        device=device,\n",
    "        image_size=IMG_SIZE,\n",
    "        run_segmentation=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = RobotArmSegFKDataset(\n",
    "        index_paths=val_index_files,\n",
    "        segformer_model_path=SegFormer_model_path, # Pass the path\n",
    "        device=device,\n",
    "        image_size=IMG_SIZE,\n",
    "        run_segmentation=True\n",
    "    )\n",
    "    \n",
    "    model = PoseEstimationHRViT(num_kp=7, pretrained=True, img_size=IMG_SIZE).to(device)\n",
    "\n",
    "    train_and_validate(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        epochs=50,\n",
    "        batch_size=4,\n",
    "        learning_rate=1e-4,\n",
    "        save_path=\"checkpoints\",\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.multiprocessing.set_ start_method('spawn', force=True)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8069281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc0ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae285cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
